{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is a simulation of a rocket that is launched from Earth and aims to land on Mars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"glowscript\" class=\"glowscript\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") { window.__context = { glowscript_container: $(\"#glowscript\").removeAttr(\"id\")};}else{ element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from time import sleep\n",
    "from random import sample\n",
    "from os import remove\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from vpython import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landing Rocket On Mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATE\n",
    "\n",
    "What's new?\n",
    "1. I take the semi-axes of the ellipses of the two planets and after each restart of the environment they start from a random place. (Here we use the ellipse equation).\n",
    "2. Calculate the velocity vector for the position of the planet after the reset of the environment.\n",
    "3. I use the law of conservation of kinetic energy and the law of conservation of momentum to change the basic velocity of the rocket.\n",
    "4. added fuel to the rocket, which will affect the reward for each step.\n",
    "5. I reduced the number of network parameters.\n",
    "\n",
    "This makes the problem more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me describe how I calculate the planet's velocity vector at a given point after a reboot. We use Kepler's second law. We know from it that for equal intervals of time a planet travels the same area speed. Let's define some constants that we will use to solve the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma$ - area velocity<br>\n",
    "$r_{x}$ - x coordinate of the radius vector<br>\n",
    "$r_{y}$ - y coordinate of the radius vector<br>\n",
    "$a$ - major semi-axis<br>\n",
    "$b$ - minor semi-axis<br>\n",
    "$\\theta$ - angle between the two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how much $\\sigma$ is because we have a basic case in which we know the speed of the planet, the radius-vector, and the angle between the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec\\sigma = \\vec v \\vec r \\sin(\\theta)$ = det $\\begin{pmatrix} r_{x} & r_{y} \\\\ v_{x} & v_{y} \\end{pmatrix}$\n",
    "\n",
    "det $\\begin{pmatrix} r_{x} & r_{y} \\\\ v_{x} & v_{y} \\end{pmatrix}$ = $r_{x}v_{y} - r_{y}v_{x}$ = $|\\vec\\sigma|$\n",
    "\n",
    "$\\Rightarrow v_{y}(v_{x}) = \\frac{\\sigma + r_{y}v_{x}}{r_{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð¢he two vectors form a parallelogram, I express the small diagonal of the parallelogram in two ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d_{2}^2 = (r_{x} - v_{x})^2 + (r_{y} - v_{y})^2 = (r_{x} - v_{x})^2 + (r_{y} - \\frac{\\sigma + r_{y}v_{x}}{r_{x}})^2$<br><br>\n",
    "$d_{2}^2 = \\vec r^2 + \\vec v^2 - 2|\\vec r||\\vec v|\\cos(\\theta) = \\vec r^2 + v_{x}^2 + (\\frac{\\sigma + r_{y}v_{x}}{r_{x}})^2 - 2|\\vec r|\\sqrt {v_{x}^2 + (\\frac{\\sigma + r_{y}v_{x}}{r_{x}})^2}\\cos(\\theta)$<br><br>\n",
    "$\\Rightarrow \\frac{r_{y}}{r_{x}}(\\sigma+r_{y}v_{x}) + r_{x}v{x} = |\\vec r|\\sqrt {v_{x}^2 + (\\frac{\\sigma + r_{y}v_{x}}{r_{x}})^2}\\cos(\\theta)$<br><br>\n",
    "We know that $|\\vec\\sigma| = |\\vec r|\\sqrt {v_{x}^2 + (\\frac{\\sigma + r_{y}v_{x}}{r_{x}})^2}\\sin(\\theta)$<br><br>\n",
    "$\\Rightarrow \\tan(\\theta)(\\frac{r_{y}}{r_{x}}(\\sigma + r_{y}v_{x}) + r_{x}v_{x}) = \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line on which the velocity vector lies is tangent to the ellipse.<br>\n",
    "Let $y = k_{1}x + c_{1}$ is the equation of the line of velocity vector.<br>\n",
    "$\\Rightarrow a^2k_{1}^2 + b^2 = c_{1}^2, c_{1} = \\sqrt{a^2k_{1}^2 + b^2}$<br>\n",
    "After replacement we get:<br>\n",
    "$k_{1} = \\frac{r_{x}r_{y}}{r_{x}^2-a^2}$<br>\n",
    "Let $y = k_{2}x + c_{2}$ is the equation of line of radius-vector.<br>\n",
    "Since the line passes through the center of the coordinate system, this means that $c_{2} = 0$ and $k_{2} = \\frac{r_{y}}{r_{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tan(\\theta) = \\frac{k_{2} - k_{1}}{1 + k_{1}k_{2}} = a^2r_{y}(a^2-r^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tan(\\theta)(\\frac{r_{y}}{r_{x}}(\\sigma + r_{y}v_{x}) + r_{x}v_{x}) = \\sigma$<br>\n",
    "$a^2r_{y}(a^2-r^2)(\\frac{r_{y}}{r_{x}}(\\sigma + r_{y}v_{x}) + r_{x}v_{x}) = \\sigma$<br><br>\n",
    "$\\Rightarrow v_{x} = \\frac{\\sigma r_{x}}{a^2 r^2 r_{y}(a^2 - r^2)} - \\frac{r_{y}}{r^2}\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec v = (\\frac{\\sigma r_{x}}{a^2 r^2 r_{y}(a^2 - r^2)} - \\frac{r_{y}}{r^2}\\sigma, \\frac{\\sigma + r_{y}v_{x}}{r_{x}}, 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both momentum and kinetic energy are conserved, so for bodies with masses $m_{1}$and $m_{2}$, and velocities $u_{1}$ and $u_{2}$ before collision, and $v_{1}$ and $v_{2}$ after collision. The momentum before and after the collision is expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_{1}u_{1} + m_{2}u_{2} = m_{1}v_{1} + m_{2}v_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kinetic energy is expressed by:<br>\n",
    "$\\frac{m_{1}u_{1}}{2}+\\frac{m_{2}u_{2}}{2} = \\frac{m_{1}v_{1}}{2} + \\frac{m_{2}v_{2}}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the system is:<br>\n",
    "$v_{1} = \\frac{m_{1}-m_{2}}{m_{1}+m_{2}}u_{1} + \\frac{2m_{2}}{m_{1}+m_{2}}u_{2}$<br>\n",
    "$v_{2} = \\frac{2m_{1}}{m_{1}+m_{2}}u_{1} + \\frac{m_{2}-m_{1}}{m_{1}+m_{2}}u_{2}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $m_{1} \\ll m_{2}$:<br>\n",
    "$ v_{1} \\approx -u_{1} + 2u_{2} $<br>\n",
    "$ v_{2} \\approx u_{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginnig let us define the constants we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to specify that the parameters of the planets and the sun will not be real, because the simulation will not work well enough to show the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grav_const = 6.67e-11\n",
    "dt = 1\n",
    "\n",
    "EARTH_RADIUS = 20\n",
    "MARS_RADIUS = 17\n",
    "SUN_RADIUS = 55\n",
    "\n",
    "max_number_of_steps = 50\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 18\n",
    "replay_size = 100\n",
    "learning_rate = 0.8\n",
    "epsilon = 1\n",
    "\n",
    "MEAN_REWARD_BOUND = 30\n",
    "\n",
    "eps_start = 0.5\n",
    "eps_decay = .99\n",
    "eps_min = 0.02\n",
    "\n",
    "fuel = 100\n",
    "\n",
    "sigma_earth = mag(vector(200, 0, 0)) * mag(vector(0, -8, 0))\n",
    "sigma_mars = mag(vector(282, 0, 0)) * mag(vector(0, -8 * 0.8, 0))\n",
    "\n",
    "def vx(pos, a, b, sigma):\n",
    "    return (sigma * pos.x) / (a*a*mag(pos)**2*pos.y*(a*a-mag(pos)**2)) - pos.y / mag(pos)**2 * sigma\n",
    "\n",
    "def vy(pos, sigma, x):\n",
    "    return (sigma + pos.y * x) / pos.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nutshell, the constants are as follows:<br><hr>\n",
    "`grav_const` - the gravitational constant of the law of universal gravitation<br>\n",
    "`dt` - the change in time by one step<br><hr>\n",
    "`EARTH_RADIUS` - radius of Earth<br>\n",
    "`MARS_RADIUS` - radius of Mars<br>\n",
    "`SUN_RADIUS` - radius of the sun<br><hr>\n",
    "`gamma` - the constant we will use when changing q-values<br>\n",
    "`batch_size` - the amount of information we need to have to start training the network<br>\n",
    "`replay_size` - the maximum amount of stored information<br>\n",
    "`learning_rate` - learning rate<br>\n",
    "`epsilon` - coefficient for a random event<br><hr>\n",
    "`eps_start` - the initial value of `epsilon`<br>\n",
    "`eps_decay` - the rate of change of the random event<br>\n",
    "`eps_min` - minimum value of the random event<br><hr>\n",
    "`MEAN_REWARD_BOUND` - the minimum reward for completing the network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input((400, 640, 4)),\n",
    "    Conv2D(filters = 64, kernel_size = 3, padding = 'valid', activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(filters = 32, kernel_size = 3, padding = 'valid', activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters = 16, kernel_size = 3, padding = 'valid', activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(filters = 8, kernel_size = 3, padding = 'valid', activation = 'relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(16, activation = 'relu'),\n",
    "    Dense(8, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of 3 convolutional layers, MaxPool for image size reduction, BatchNormalization for scaling and 3 hidden layers. The reason we have 8 output neurons is that we have 8 possible directions for movement in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 398, 638, 64)      2368      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 199, 319, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 197, 317, 32)      18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 98, 158, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 98, 158, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 156, 16)       4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 48, 78, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 46, 76, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 23, 38, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6992)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                447552    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 477,040\n",
      "Trainable params: 476,976\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate), loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two classes, using the classes from `vpython` to make our work with data more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two methods in the `Planet` class that serve to change the position of the planets and change the speed of the planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Star:\n",
    "    def __init__(self, mass, radius, pos, color):\n",
    "        self.star = sphere(pos = pos, radius = radius, color = color)\n",
    "        self.mass = mass\n",
    "\n",
    "class Planet:\n",
    "    def __init__(self, mass, radius, pos, velocity, color):\n",
    "        self.planet = sphere(pos = pos, radius = radius, color = color, make_trail = False)\n",
    "        self.mass = mass\n",
    "        if radius == EARTH_RADIUS:\n",
    "            self.planet.texture = textures.earth\n",
    "        self.planet.v = velocity\n",
    "        self.momentum = mag(velocity) * mass\n",
    "        \n",
    "    def update_params(self, Fgrav):\n",
    "        # self.momentum += Fgrav * dt\n",
    "        # print('Velocity: ', self.planet.v, 'Pos: ', self.planet.pos)\n",
    "        self.planet.v += Fgrav * dt # / self.mass\n",
    "    \n",
    "    def move(self):\n",
    "        self.planet.pos += self.planet.v * dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below, we create our environment, and then we create a `reset_env()` function so we can restart everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`total_reward` is the reward for the current performance, and `total_steps` is the number of steps we have taken since the rocket took off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dist_from_sun` is a dictionary that stores information about the distances from the sun to Earth, Mars and the rocket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") {require.undef(\"nbextensions/vpython_libraries/glow.min\");}else{element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") {require.undef(\"nbextensions/vpython_libraries/glowcomm\");}else{element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") {require.undef(\"nbextensions/vpython_libraries/jquery-ui.custom.min\");}else{element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") {require([\"nbextensions/vpython_libraries/glow.min\"], function(){console.log(\"GLOW LOADED\");});}else{element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") {require([\"nbextensions/vpython_libraries/glowcomm\"], function(){console.log(\"GLOWCOMM LOADED\");});}else{element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (typeof Jupyter !== \"undefined\") {require([\"nbextensions/vpython_libraries/jquery-ui.custom.min\"], function(){console.log(\"JQUERY LOADED\");});}else{element.textContent = ' ';}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "\n",
    "celestial_bodies = {\n",
    "    'Sun': Star(1e10/2, SUN_RADIUS, vector(0, 0, 0), color.orange),\n",
    "    'Earth': Planet(1e5/3, EARTH_RADIUS, vector(200, 0, 0), vector(0, -8, 0), color.blue),\n",
    "    'Mars': Planet(1e4/3, MARS_RADIUS, vector(282, 0, 0), vector(0, -8*0.8, 0), color.red)\n",
    "}\n",
    "    \n",
    "dist_from_sun = {}\n",
    "    \n",
    "rocket = cylinder(\n",
    "    pos = vector(200, EARTH_RADIUS, 0), \n",
    "    axis = vector(0, 3, 0), \n",
    "    length = 11,\n",
    "    velocity = vector(0, 0, 0),\n",
    "    radius = 3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_axis_earth = 0\n",
    "minor_axis_earth = 0\n",
    "\n",
    "major_axis_mars = 0\n",
    "minor_axis_mars = 0\n",
    "\n",
    "def collected_info(a1, b1, a2, b2):\n",
    "    if a1 != 0 and b1 != 0 and a2 != 0 and b2 != 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    simulation()\n",
    "    \n",
    "    if collected_info(major_axis_earth, minor_axis_earth, major_axis_mars, minor_axis_mars):\n",
    "        break\n",
    "    \n",
    "    if -5 < celestial_bodies['Earth'].planet.pos.x < 5:\n",
    "        minor_axis_earth = abs(celestial_bodies['Earth'].planet.pos.y)\n",
    "    if -5 < celestial_bodies['Earth'].planet.pos.y < 5 and celestial_bodies['Earth'].planet.pos.x < 0:\n",
    "        major_axis_earth = (200 + abs(celestial_bodies['Earth'].planet.pos.x)) / 2\n",
    "    \n",
    "    if -5 < celestial_bodies['Mars'].planet.pos.x < 5:\n",
    "        minor_axis_mars = abs(celestial_bodies['Mars'].planet.pos.y)\n",
    "    if -5 < celestial_bodies['Mars'].planet.pos.y < 5 and celestial_bodies['Mars'].planet.pos.x < 0:\n",
    "        major_axis_mars = (282 + abs(celestial_bodies['Mars'].planet.pos.x)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((235.8012320191254, 226.33431532390125),\n",
       " (293.45783811303306, 289.7154496903096))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(major_axis_earth, minor_axis_earth), (major_axis_mars, minor_axis_mars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_earth = lambda x: minor_axis_earth / major_axis_earth * math.sqrt((200 - x) * (2 * major_axis_earth + x - 200))\n",
    "y_mars = lambda x: minor_axis_mars / major_axis_mars * math.sqrt((282 - x) * (2 * major_axis_mars + x - 282))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the above code, here we also have a return to the initial values of some of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env():\n",
    "    \n",
    "    global total_reward, total_steps, epsilon, frame, eps_start, fuel\n",
    "    \n",
    "    eps_start *= eps_decay\n",
    "    epsilon = eps_start / 2\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    frame = 0\n",
    "    fuel = 100\n",
    "    \n",
    "    global celestial_bodies\n",
    "    for celestial_body in celestial_bodies.values():\n",
    "        try:\n",
    "            celestial_body.planet.visible = False\n",
    "        except:\n",
    "            celestial_body.star.visible = False\n",
    "    try:        \n",
    "        x_start_earth = np.random.randint(200 - 2 * major_axis_earth, 200)\n",
    "        print('x_start_earth: ', x_start_earth)\n",
    "        x_start_mars = np.random.randint(282 - 2 * major_axis_mars, 282)\n",
    "    except:\n",
    "        reset_env()\n",
    "        return\n",
    "    \n",
    "    celestial_bodies = {\n",
    "        'Sun': Star(1e10/2, SUN_RADIUS, vector(0, 0, 0), color.orange),\n",
    "        'Earth': Planet(1e5/3, EARTH_RADIUS, vector(x_start_earth, y_earth(x_start_earth), 0), vector(0, -8, 0), color.blue),\n",
    "        'Mars': Planet(1e4/3, MARS_RADIUS, vector(x_start_mars, y_mars(x_start_mars), 0), vector(0, -8*0.8, 0), color.red)\n",
    "    }\n",
    "    \n",
    "    celestial_bodies['Earth'].planet.v = vector(vx(celestial_bodies['Earth'].planet.pos, major_axis_earth, minor_axis_earth, sigma_earth), vy(celestial_bodies['Earth'].planet.pos, sigma_earth, vx(celestial_bodies['Earth'].planet.pos, major_axis_earth, minor_axis_earth, sigma_earth)), 0)\n",
    "    celestial_bodies['Mars'].planet.v = vector(vx(celestial_bodies['Mars'].planet.pos, major_axis_mars, minor_axis_mars, sigma_mars), vy(celestial_bodies['Mars'].planet.pos, sigma_mars, vx(celestial_bodies['Mars'].planet.pos, major_axis_mars, minor_axis_mars, sigma_mars)), 0)\n",
    "    \n",
    "    global dist_from_sun\n",
    "    dist_from_sun = {}\n",
    "    \n",
    "    global rocket\n",
    "    rocket.visible = False\n",
    "    rocket = cylinder(\n",
    "        pos = vector(x_start_earth, y_earth(x_start_earth) + EARTH_RADIUS, 0), \n",
    "        axis = vector(0, 3, 0), \n",
    "        length = 11, \n",
    "        radius = 3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_start_earth:  149\n"
     ]
    }
   ],
   "source": [
    "reset_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Experience` is the class through which we will keep information about the `reward` we have received after performing an `action` from position `state` to position `next_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward', 'done')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReplayMemory` is the class in which we will store the objects of the `Experience` class, as the maximum stored information is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "        \n",
    "    def push(self, experience):\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace = False)\n",
    "        states, actions, next_states, rewars, dones = zip([self.memory[ind] for ind in indices])\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `push` function adds an object to the `memory` list, and if it is full, it starts to fill from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample()` function takes n number of random examples from `memory`, where n = `batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below we create the class `Agent`, which has the functions for taking action - `play_step()`, performing action - `step()` and the function for training our network - `train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, exp_buffer):\n",
    "        self.exp_buffer = exp_buffer\n",
    "        \n",
    "    def calculate_dist_to_sun(position):\n",
    "        return mag(rocket.pos)\n",
    "    \n",
    "    def landing():\n",
    "        dist = mag(rocket.pos - celestial_bodies['Mars'].planet.pos)\n",
    "        \n",
    "        if dist <= celestial_bodies['Mars'].planet.radius:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def dist_mars_rocket():\n",
    "        return mag(rocket.pos - celestial_bodies['Mars'].planet.pos)\n",
    "    \n",
    "    def landed_on_earth():\n",
    "        dist = ((celestial_bodies['Earth'].planet.pos.x - rocket.pos.x)**2 +\n",
    "               (celestial_bodies['Earth'].planet.pos.y - rocket.pos.y)**2 +\n",
    "               (celestial_bodies['Earth'].planet.pos.z - rocket.pos.z)**2)**0.5\n",
    "        \n",
    "        if dist <= celestial_bodies['Earth'].planet.radius:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def train(self, net, target_net):\n",
    "        batch = sample(self.exp_buffer.memory, batch_size)\n",
    "        \n",
    "        for state, action, next_state, reward, done in batch:\n",
    "            \n",
    "            target = net.predict(np.array([state]))\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = target_net.predict(np.array([next_state]))\n",
    "                target[0][action] = reward + gamma * np.amax(t)\n",
    "                \n",
    "            net.fit(np.array([state]), np.array(target), epochs=1, verbose=0)\n",
    "        \n",
    "    def step(action):\n",
    "        \n",
    "        global frame_idx, total_steps, frame, fuel\n",
    "        is_done = False\n",
    "        \n",
    "        d_dist_before_action = Agent.dist_mars_rocket()\n",
    "        \n",
    "        rocketVelocity = vector(0, 0, 0)\n",
    "        \n",
    "        if action == 0:\n",
    "            rocketVelocity.x -= 10\n",
    "            rocketVelocity.y += 10\n",
    "        elif action == 1:\n",
    "            rocketVelocity.y += 10\n",
    "        elif action == 2:\n",
    "            rocketVelocity.x += 10\n",
    "            rocketVelocity.y += 10\n",
    "        elif action == 3:\n",
    "            rocketVelocity.x -= 10\n",
    "        elif action == 4:\n",
    "            rocketVelocity.x += 10\n",
    "        elif action == 5:\n",
    "            rocketVelocity.x -= 10\n",
    "            rocketVelocity.y -= 10\n",
    "        elif action == 6:\n",
    "            rocketVelocity.y -= 10\n",
    "        elif action == 7:\n",
    "            rocketVelocity.x += 10\n",
    "            rocketVelocity.y -= 10\n",
    "            \n",
    "        rocketVelocity = -rocketVelocity + 2 * celestial_bodies['Earth'].planet.v\n",
    "        rocketVelocity = -rocketVelocity + 2 * celestial_bodies['Mars'].planet.v\n",
    "        \n",
    "        fuel -= mag(rocketVelocity) * 0.02\n",
    "            \n",
    "        rocket.pos += rocketVelocity\n",
    "            \n",
    "        d_dist_after_action = Agent.dist_mars_rocket()\n",
    "        \n",
    "        reward = -0.01 * d_dist_after_action\n",
    "        \n",
    "        if d_dist_after_action < d_dist_before_action:\n",
    "            reward *= -1\n",
    "            reward = 3 - reward\n",
    "        \n",
    "        if reward < -3:\n",
    "            is_done = True\n",
    "            reward = reward - (max_number_of_steps - total_steps) * 6\n",
    "        \n",
    "        reward -= (1 - fuel * 0.02)\n",
    "        print(reward, fuel)\n",
    "        \n",
    "        total_steps += 1\n",
    "        \n",
    "        if Agent.calculate_dist_to_sun(rocket.pos) < celestial_bodies['Sun'].star.radius:\n",
    "            reward -= 150\n",
    "            is_done = True\n",
    "        elif Agent.landed_on_earth():\n",
    "            reward -= 20\n",
    "            is_done = True\n",
    "        elif Agent.landing():\n",
    "            reward += 150\n",
    "            is_done = True\n",
    "        \n",
    "        scene.capture('planets_positions{}'.format(frame_idx))\n",
    "        sleep(1)\n",
    "        next_state = imread('.\\\\Planets Positions - Images\\\\planets_positions{}.png'.format(frame_idx))\n",
    "        next_state = next_state / 255\n",
    "        remove('.\\\\Planets Positions - Images\\\\planets_positions{}.png'.format(frame_idx))\n",
    "        return next_state, reward, is_done\n",
    "            \n",
    "        \n",
    "    def play_step(self, net, epsilon, state):\n",
    "        \n",
    "        global total_reward\n",
    "        \n",
    "        action = 0\n",
    "        done_reward = None\n",
    "        \n",
    "        if epsilon >= np.random.random():\n",
    "            action = np.random.randint(8)\n",
    "        else:\n",
    "            action = np.argmax(net.predict(np.array([state])))\n",
    "        \n",
    "        new_state, reward, is_done = Agent.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        exp = Experience(state, action, new_state, reward, is_done)\n",
    "        self.exp_buffer.push(exp)\n",
    "        self.state = new_state\n",
    "        if is_done or total_steps == max_number_of_steps:\n",
    "            done_reward = total_reward\n",
    "            reset_env()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><code>play_step()</code>: this function selects the action to take in the given situation. The action can be chosen randomly or the one with the highest value in the neural network. We save the results of each step and if we are ready we start again.</li>\n",
    "    <li><code>step()</code>: the function performs the set action and receives the corresponding reward. The reward, the next_state of the environment and information on whether we have graduated are returned. Here's a little more information:</li>\n",
    "    Rewards can vary greatly. Therefore, they are briefly described here:<br>\n",
    "    1. <code>reward = -0.01 * Agent.dist_mars_rocket()</code> when the rocket moves away from Mars, it receives a negative reward.<br>\n",
    "    2. <code>reward *= -1; reward = 3 - reward</code> when it's closer the reward is positive.<br>\n",
    "    3. <code>reward = reward - (max_number_of_steps - total_steps) * 6</code> When our agent is more than 400 pixels from Mars (too far), he receives the following reward. It is because the later it moves away, the less the rocket is \"punished\", and the earlier, the more points are taken away from it. max_number_of_steps = 50 is the maximum number of moves per one attempt in the environment.<br>\n",
    "    4. <code>reward -= 20</code> this is the case when the agent lands on the sun or on the ground of Earth again.<br>\n",
    "    5. <code>reward -= 150</code> - the case when rocket hits the sun.<br>\n",
    "    6. <code>reward += 150</code> when the rocket reaches its destination - Mars.\n",
    "    <li><code>train()</code>: we choose a number of random examples (with the size of <code>batch_size</code>) from our memory - <code>ReplayMemory</code> and train the neural network to predict a future event based on the current one. Before training we take q-values from the prediction of actions from the neural network <code>net</code>. We replace the selected <code>action</code> with the <code>reward</code> if we are ready, otherwise we replace it with the sum of the <code>reward</code> and the further <code>action</code> we predict with <code>target_net</code>.</li>\n",
    "</ul>\n",
    "The other functions are clear - they compare the position of the rocket with the celestial bodies and help choose the <code>reward</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create other variables that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    net, target_net = tf.keras.models.load_model('.\\\\weights.sav'), tf.keras.models.load_model('.\\\\weights.sav')\n",
    "except:\n",
    "    net, target_net = model, model\n",
    "target_net.set_weights(net.get_weights())\n",
    "\n",
    "buffer = ReplayMemory(replay_size)\n",
    "agent = Agent(buffer)\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "frame = 0\n",
    "frame_idx = 0\n",
    "best_mean_reward = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`net` and `target_net` are the two neural networks. `net` is learned using `target_net`, and then the weights of `target_net` are equated to `net`.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`buffer` is an object of type `ReplayMemory` - the place where we will store information.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`agent` is our agent (not the rocket, but the place where we store some important information and functions about the rocket).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`total_rewards` is list with all of the rewards.<br>\n",
    "`best_mean_reward` is the best mean reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`frame` is the number of steps that we are taking.<br>\n",
    "`frame_idx` is total number of frames.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    rate(8)\n",
    "\n",
    "    dist_from_sun['Earth'] = (celestial_bodies['Earth'].planet.pos.x**2 + celestial_bodies['Earth'].planet.pos.y**2 + celestial_bodies['Earth'].planet.pos.z**2)**0.5\n",
    "    dist_from_sun['Mars'] = (celestial_bodies['Mars'].planet.pos.x**2 + celestial_bodies['Mars'].planet.pos.y**2 + celestial_bodies['Mars'].planet.pos.z**2)**0.5\n",
    "\n",
    "    radialVector = {\n",
    "        'Earth': (celestial_bodies['Sun'].star.pos - celestial_bodies['Earth'].planet.pos) / dist_from_sun['Earth'],\n",
    "        'Mars': (celestial_bodies['Sun'].star.pos - celestial_bodies['Mars'].planet.pos) / dist_from_sun['Mars']\n",
    "    }\n",
    "    \n",
    "    Fgrav_earth_sun = grav_const * celestial_bodies['Earth'].mass * celestial_bodies['Sun'].mass * radialVector['Earth'] / dist_from_sun['Earth']**2\n",
    "    Fgrav_mars_sun = 10 * grav_const * celestial_bodies['Mars'].mass * celestial_bodies['Sun'].mass * radialVector['Mars'] / dist_from_sun['Mars']**2\n",
    "    Fgrav_earth_mars = grav_const * celestial_bodies['Earth'].mass * celestial_bodies['Mars'].mass * (celestial_bodies['Earth'].planet.pos - celestial_bodies['Mars'].planet.pos) / mag(celestial_bodies['Earth'].planet.pos - celestial_bodies['Mars'].planet.pos)**3\n",
    "    \n",
    "    celestial_bodies['Earth'].update_params(Fgrav_earth_sun + Fgrav_earth_mars)\n",
    "    celestial_bodies['Mars'].update_params(Fgrav_mars_sun + Fgrav_earth_mars)\n",
    "    \n",
    "    celestial_bodies['Earth'].move()\n",
    "    celestial_bodies['Mars'].move()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `simulation()` function is a simple execution of the animation of the motion of celestial bodies. Newton's law of universal gravitaion is used. Earth and Mars move in an ellipse around the sun, which is Kepler's first law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code starts the whole program. At each step we increase `frame` and `frame_idx`, as well as change the value of `epsilon`. We take one step of the simulation and take the image of the environment. We scale the image and perform a step. If we have enough data, we train the networks. At the end of the simulation we add the reward to `total_rewards` and if the mean reward from the last 10 performances is better than `best_mean_reward` we keep the weights of net. Training continues until the `mean_reward` of the last 10 times exceeds `MEAN_REWARD_BOUND`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8285020618580177 94.46598691246702\n",
      "-1.7947147998210176 94.40363985944244\n",
      "-1.9946143166999926 93.93227088531343\n",
      "-1.9707262250552162 93.87408049500168\n",
      "-170.17892424381003 93.39176992069564\n",
      "x_start_earth:  61\n",
      "25: 3 games, mean_reward -172.244, (epsilon 0.24)\n",
      "1.1847516613855178 99.90907498724806\n",
      "x_start_earth:  -115\n",
      "26: 4 games, mean_reward -133.887, (epsilon 0.24)\n",
      "2.9878356308367824 99.85764009832641\n",
      "2.9694191908952456 99.64876882608867\n",
      "2.9353504879901875 99.43730266288921\n",
      "2.989550431395176 99.16596548933444\n",
      "2.9452789066239555 98.94912675530746\n",
      "-0.27504713453851837 98.63536755715616\n",
      "2.6637514690763187 98.41295259691695\n",
      "-0.4101360942441896 98.18768741388693\n",
      "2.5822439019714514 98.04662915256431\n",
      "-0.5085488165349457 97.81556607745291\n",
      "-0.6095189980188978 97.58156546463835\n",
      "-0.7194971505992227 97.34460803482926\n",
      "-0.8374431262303381 97.1046791268908\n",
      "-0.9624559818688223 96.86176853298521\n",
      "-1.0937678001407258 96.61587034216303\n",
      "-1.2307297126452523 96.36698279254436\n",
      "-1.372795292471224 96.11510813210865\n",
      "-1.5195041621177328 95.86025248801243\n",
      "-1.6704671645969678 95.60242574427419\n",
      "-1.8253535776122674 95.34164142760176\n",
      "-1.983880395780444 95.0779166010887\n",
      "1.025886874588693 94.77410521248756\n",
      "-170.14754589616666 94.55629302496246\n",
      "x_start_earth:  -159\n",
      "49: 5 games, mean_reward -139.923, (epsilon 0.24)\n",
      "-0.37135381157740266 99.58720395519161\n",
      "-0.37874001078616404 99.17192423874889\n",
      "-0.41368644694350265 98.75420028376425\n",
      "-0.4755157379302519 98.33407193063577\n",
      "-0.5621948340500744 97.91157941705578\n",
      "-0.6708731625508375 97.48676337081955\n",
      "-0.7984421595466342 97.05966480537148\n",
      "-0.9419397035357298 96.63032511801111\n",
      "-1.0987617651222397 96.19878609068579\n",
      "-1.2667272812797337 95.76508989330034\n",
      "-1.444059358493774 95.32927908947799\n",
      "-237.24991744293314 94.87146031635335\n",
      "x_start_earth:  189\n",
      "118: 6 games, mean_reward -157.548, (epsilon 0.23)\n",
      "0.5746712271998948 99.63856179353247\n",
      "x_start_earth:  -126\n",
      "119: 7 games, mean_reward -137.816, (epsilon 0.23)\n",
      "-303.1567415298986 99.41824922719326\n",
      "x_start_earth:  -105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-523b45a52ee1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-f3cf29313f99>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, net, target_net)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    frame += 1\n",
    "    frame_idx += 1\n",
    "    epsilon = max(epsilon * eps_decay, eps_min)\n",
    "    \n",
    "    simulation()\n",
    "    \n",
    "    scene.capture('planets_positions{}'.format(frame_idx))\n",
    "    sleep(1)\n",
    "    \n",
    "    try:\n",
    "        state = imread('.\\\\Planets Positions - Images\\\\planets_positions{}.png'.format(frame_idx))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    state = state / 255.0\n",
    "    \n",
    "    remove('.\\\\Planets Positions - Images\\\\planets_positions{}.png'.format(frame_idx))\n",
    "    \n",
    "    reward = agent.play_step(net, epsilon, state)\n",
    "    \n",
    "    if len(buffer.memory) > batch_size:\n",
    "        agent.train(net, target_net)\n",
    "    \n",
    "    if reward is not None:\n",
    "        target_net.set_weights(net.get_weights())\n",
    "        total_rewards.append(reward)\n",
    "        mean_reward = np.mean(total_rewards[-10:])\n",
    "        print('%d: %d games, mean_reward %.3f, (epsilon %.2f)' % \n",
    "            (frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "        \n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            net.save('weights.sav')\n",
    "            best_mean_reward = mean_reward\n",
    "            print('Best mean reward updated %.3f' % (best_mean_reward))\n",
    "                \n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print('Solved in {} frames!'.format(frame_idx))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weights.sav\\assets\n"
     ]
    }
   ],
   "source": [
    "net.save('weights.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for your attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
